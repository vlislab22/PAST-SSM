<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>PAST-SSM</title>
    <meta name="author" content="Jiazhou Zhou">
    <meta name="description" content="Project page of PAST-SSM">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>

  </head>

  <body style=“text-align: center;”>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Path-adaptive Spatio-Temporal State Space Model for Event-based Recognition with Arbitrary Duration<br /> 
<!--                 <small>
                    
                </small> -->
            </h1>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
			<img src="./image/jiazhou.jpg" height="80px"><br>
                        <a href="https://jiazhou-garland.github.io/" >
                         Jiazhou Zhou
                        </a>
                        
                        <br /> AI Thrust, HKUST(GZ)
			<br /> International Digital Economy Academy (IDEA)
                        <br /> &nbsp &nbsp

                    </li>

                    <li>
                        <img src="./images/khao.jpg" height="80px"><br>
                        <a href="https://scholar.google.com/citations?user=IwvcylUAAAAJ&hl=zh-CN" >
                            Kanghao Chen
                        </a>
                        <br /> AI Thrust, HKUST(GZ)
                        <br /> &nbsp &nbsp
                    </li>

                    <li>
			<img src="./image/leizhang.jpg" height="80px"><br>
                        <a href="https://www.leizhang.org/" >
                           Lei Zhang
                        </a>
                        <br /> International Digital Economy Academy (IDEA)
			<br /> &nbsp &nbsp
                    </li>
                  
                    <li>
                        <img src="./images/linwang.png" height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">
                            Addison Lin Wang
                        </a>
                        <br /> AI & CMA Thrust, HKUST(GZ) 
                        <br/> Dept. of CSE, HKUST 
                    </li>
                </ul>
            </div>
        </div>



        <!-- ##### Elements #####-->
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
			    <a href="https://arxiv.org/pdf/2308.03135.pdf">
                            <img src="./image/arxiv.png" height="100px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>

                            <img src="./image/youtube_icon.jpg" height="100px"><br>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/jiazhou-garland/E-CLIP">
                            <img src="./image/github_icon.jpg" height="100px"><br>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>

                        <!-- <li>
                            
                            <img src="./image/colab_icon.jpg" height="100px"><br>
                                <h4><strong>Colab</strong></h4>
                            </a>
                        </li> -->
 

                        <li>
                            <a href="https://github.com/jiazhou-garland/ELIP/blob/master/Appendix.pdf">
                            <img src="./image/slide_icon.jpg" height="100px"><br>
                                <h4><strong>Supp</strong></h4>
                            </a>
                        </li>                     
                        <li>
                            <a href="https://vlislab22.github.io/vlislab/">
                            <img src="./image/lab_logo.png" height="100px"><br>
                                <h4><strong>Vlislab</strong></h4>
                            </a>
                        </li>                       
                      
                    </ul>
                </div>
        </div>


        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
		   	Event cameras are bio-inspired sensors that capture the intensity changes asynchronously and output event streams with distinct advantages, such as high temporal resolution. 
To exploit event cameras for object/action recognition, existing methods predominantly sample and aggregate events in a second-level duration at every fixed temporal interval (or frequency).
However, they often face difficulties in capturing the spatiotemporal relationships for longer, e.g., minute-level, events and generalizing across varying temporal frequencies. 
To fill the gap, we present a novel framework, dubbed PAST-SSM, exhibiting superior capacity in recognizing events of arbitrary duration e.g., 0.1s to 4.5min and generalizing to varying inference frequencies.
Our key insight is to learn the spatiotemporal relationships from the encoded event features via the state space model (SSM) -- whose linear complexity makes it ideal for modeling high temporal resolution events with longer sequences. 
To achieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan (PEAS) module to encode events of varying duration into features with fixed dimensions by adaptively scanning and selecting aggregated event frames. 
On top of PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to minimize the randomness and redundancy of the encoded features. This subtly enhances the model generalization across different inference frequencies. 
Lastly, the SSM is employed to better learn the spatiotemporal properties from the encoded features. 
Moreover, we build a minute-level event-based recognition dataset, named ArDVS100, with arbitrary duration for the benefit of the community. 
Extensive experiments prove that our method outperforms prior arts by +3.45%, +0.38% and +8.31% on the DVS Action, SeAct, and HARDVS datasets, respectively. 
In addition, it achieves an accuracy of 97.35%, 89.00%, and 100.00% in our ArDVS100, TemArDVS100, and Real-ArDVS10 datasets respectively with the duration from 1s to 265s. 
Our method also shows strong generalization with a maximum accuracy drop of only 8.62% for varying inference frequencies while the baseline's drop reaches 27.59%.
                </p>
            </div>
        </div>

 

        <!-- ##### Results #####-->

     <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Overall framework of our PAST-SSM
            </h3>
            <img src="./image/framework.png" class="img-responsive" alt="vis_res"  class="center" >
      	</div>
    </div>
<!-- ####      <div class="col-md-8 col-md-offset-2">
          <h3>
              Comparison on octree representations
          </h3>
          <p class="text-justify">
            DOT shows the more compact structure of DOT, 
            resulting in fewer ray intersections, explaining our significant rendering speed boost.

          </p>
		  <img src="./image/dot_cp.png" class="img-responsive" alt="vis_res" class="center"><br>
    </div>   
    <div class="col-md-8 col-md-offset-2">
        <h3>
            Comparison on visual quality and memory consumption
        </h3>
        <p class="text-justify">
            DOT provides more details in complex regions, such as sharper reflections on windows and more evident edges on fences. 

        </p>
        <img src="./image/dot_cp2.png" class="img-responsive" alt="vis_res" class="center"><br>
    </div>     
    </div>


		      
    	<div class="row">      
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Demo 
          </h3>   
    </div>   
      
    <div class="col-md-8 col-md-offset-2">

            <video width="800"  controls >
                <source src="./video/dot.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>   
    </div>          
      </div>  ####-->
   <!-- ##### BibTex #####-->
        <hr>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
 
                <div class="row align-items-center">
                    <div class="col py-3">
                        <pre class="border">             
@article{zhou2023clip,
  title={Path-adaptive Spatio-Temporal State Space Model for Event-based Recognition with Arbitrary Duration},
  author={Zhou, Jiazhou and Kanghao, Chen and Lei, Zhang and Wang, Lin},
  journal={arXiv preprint arXiv:2308.03135},
  year={2024}
}
</pre>
                    </div>
                </div>
              
    
          </div>
          
        </div>
    </div>
</body>
</html>
